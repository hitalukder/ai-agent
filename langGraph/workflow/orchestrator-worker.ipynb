{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25439bb2-f515-4ac1-a633-407ae5c70314",
   "metadata": {},
   "source": [
    "# Orchestrator-Worker\n",
    "\n",
    "**In the orchestrator-worker setup, one main LLM (the orchestrator) breaks a task into smaller parts and assigns them to other LLMs (the workers).** The orchestrator then collects and combines their responses into a final result.\n",
    "\n",
    "**When to use this approach:**\n",
    "This method is useful for complex tasks where you can't predict all the steps in advance. For example, in coding, the number of files to change and what to change depends on the task itself. Unlike parallelization (where tasks are split up ahead of time), the orchestrator-worker model is more flexibleâ€”it figures out what needs to be done based on the input.\n",
    "\n",
    "## Prerequisite\n",
    "\n",
    "- [Ollama](https://ollama.com/) installed to generate the graph\n",
    "- Taviley search tool to retrieve context for our usecase.\n",
    "\n",
    "Download the granite model\n",
    "\n",
    "```sh\n",
    "ollama pull granite3.2:2b  \n",
    "```\n",
    "\n",
    "## Usecase\n",
    "Since orchestrator-worker workflows are common, LangGraph provides a Send API to make them easier to build.\n",
    "It allows you to create worker nodes on the fly and send each one a specific input. Each worker keeps its own state, and their results are stored in a shared location that the orchestrator can access. This way, the orchestrator can see all the worker outputs and combine them into a final result.\n",
    "\n",
    "In the example below, we loop through a list of sections and use the Send API to send each section to a separate worker node. I took this example from langgraph official website and implemented. For your reference please follow the link [here](https://langchain-ai.github.io/langgraph/tutorials/workflows/#orchestrator-worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdd8690-38dd-4145-bbe3-491be6cd697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr\n",
    "## lets start with installing all the required libraries\n",
    "%pip install --upgrade --quiet langgraph  langchain langchain-community langchain-ollama langchain-experimental python-dotenv langchain_core tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540826bc-5ce7-468c-8bec-1775317de171",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Load the environment variable from .env file\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda7ceae-d4a9-4b43-97f2-4671a8ab4565",
   "metadata": {},
   "source": [
    "### LLM Model using OLLAMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43348ffc-3f89-45f6-b056-7fadfe60183f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LLM\n",
    "from langchain_ollama import ChatOllama\n",
    "from IPython.display import display\n",
    "\n",
    "local_llm = \"granite3.2:2b\"\n",
    "llm = ChatOllama(model=local_llm, temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7aea878-00ed-4d43-9cdd-ccb0df8252b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, List\n",
    "import operator\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )\n",
    "\n",
    "\n",
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf978b9-98b5-4870-9f28-962c36427076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sections(sections=[Section(name='Introduction', description='Briefly introduce the concept of LLM (Large Language Models) and their significance in the field of artificial intelligence. Mention the purpose of the report, which is to explore scaling laws related to these models.'), Section(name='Background on Large Language Models', description='Provide a comprehensive overview of what LLMs are, their architecture, and how they function. Discuss key milestones in the development of LLMs that have led to their current capabilities.'), Section(name='Understanding Scaling Laws', description='Define scaling laws in the context of technology and AI. Explain why these laws are crucial for understanding the performance, efficiency, and resource requirements of large language models as they grow in size or complexity.'), Section(name='Key Scaling Laws in LLMs', description='Detail several key scaling laws observed in LLMs. These could include: 1) Model Size vs. Performance, 2) Training Data Size vs. Model Accuracy, 3) Computational Resources (GPU/TPU usage) vs. Inference Speed, and 4) Memory Usage vs. Model Capacity. For each law, provide empirical evidence or research findings that support these correlations.'), Section(name='Case Studies', description='Present case studies of real-world applications where LLMs have been scaled up to demonstrate practical implications of the scaling laws discussed. This could include examples from natural language processing tasks, translation services, or other AI applications.'), Section(name='Challenges and Limitations in Scaling LLMs', description='Discuss the challenges faced when attempting to scale LLMs further. These might include computational resource constraints, energy consumption, data privacy concerns, and potential degradation of model performance due to over-scaling.'), Section(name='Future Directions and Predictions', description='Based on current trends and research, predict future directions for LLM scaling. Discuss emerging technologies or strategies that could potentially mitigate the challenges mentioned in the previous section. Also, speculate on how these advancements might impact various industries.'), Section(name='Conclusion', description='Summarize the key findings of your report. Reiterate the importance of understanding LLM scaling laws and their implications for AI development and practical applications. End with a thought-provoking statement or question about the future of large language models.')])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Generate queries\n",
    "topic=\"Create a report on LLM scaling laws\"\n",
    "\n",
    "report_sections = planner.invoke(\n",
    "    [\n",
    "        SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "        HumanMessage(content=f\"Here is the report topic: {topic}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "report_sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9efd5fa-27b8-40ad-ab33-ee1088361f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(report_sections.sections)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a982c2-04a8-4b2e-bb7d-a43c63dd859b",
   "metadata": {},
   "source": [
    "#### Graph states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c676dfa1-21ff-4126-8a01-a50d647a113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, List, Any\n",
    "\n",
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    topic: str  # Report topic\n",
    "    sections: list[Section]  # List of report sections\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # All workers write to this key in parallel\n",
    "    final_report: str  # Final report\n",
    "\n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90284e-d423-4392-8a8f-cdb005442af5",
   "metadata": {},
   "source": [
    "#### Graph nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "034765b4-725b-42ca-abd5-6d9c8415bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def orchestrator(state: State):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"sections\": report_sections.sections}\n",
    "\n",
    "## It is using worker statue\n",
    "#########################################################\n",
    "def llm_call(state: WorkerState):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    print(\"___LLM_CALL___\")\n",
    "    # Generate section\n",
    "    section = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [section.content]}\n",
    "\n",
    "def synthesizer(state: State):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_report\": completed_report_sections}\n",
    "\n",
    "\n",
    "from langgraph.constants import Send\n",
    "\n",
    "# Conditional edge function to create llm_call workers that each write a section of the report\n",
    "def assign_workers(state: State):\n",
    "    \"\"\"Assign a worker to each section in the plan\"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd15e8-983d-4039-a570-74a8d390c14d",
   "metadata": {},
   "source": [
    "#### Build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "950707ef-5759-419b-bce8-0e2500144ea1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Build workflow\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "\n",
    "# Add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    \"orchestrator\", assign_workers, [\"llm_call\"]\n",
    ")\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "# Compile the workflow\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "\n",
    "# Show the workflow\n",
    "display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e704cae-48e1-4a4a-bf1c-4154d31f2cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___LLM_CALL______LLM_CALL___\n",
      "\n",
      "___LLM_CALL___\n",
      "___LLM_CALL___\n",
      "___LLM_CALL___\n",
      "___LLM_CALL___\n",
      "___LLM_CALL___\n",
      "___LLM_CALL___\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Section: **Understanding Large Language Models (LLMs): A Pivotal Force in AI**\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Large Language Models (LLMs) represent a significant leap forward in the realm of artificial intelligence, marking an era where machines can generate human-like text with remarkable proficiency. These models are not mere tools for text generation; they are sophisticated computational constructs that emulate and extend human language capabilities, enabling them to comprehend, translate, summarize, and even create coherent narratives across a vast array of domains.\n",
       "\n",
       "### The Genesis of LLMs\n",
       "\n",
       "The genesis of LLMs can be traced back to transformer architectures introduced in the groundbreaking 2017 paper \"Attention Is All You Need\" by Vaswani et al. These models, fundamentally different from their predecessors, employed a mechanism called 'attention' to weigh the importance of input words when producing an output wordâ€”a paradigm shift in how information flow was managed within neural networks for language tasks.\n",
       "\n",
       "### Significance in AI Landscape\n",
       "\n",
       "In the contemporary AI landscape, LLMs have become indispensable components, driving advancements across several critical areas:\n",
       "\n",
       "- **Natural Language Understanding (NLU):** By understanding context and semantics, LLMs excel at interpreting human language, enabling applications like sentiment analysis, question answering systems, and chatbots.\n",
       "\n",
       "- **Natural Language Generation (NLG):** They can craft coherent sentences and paragraphs, powering content creation tools, automated summarization services, and even creative writing assistants.\n",
       "\n",
       "- **Machine Translation:** LLMs have revolutionized translation services by understanding source languages deeply and generating accurate translations in target languages.\n",
       "\n",
       "### Purpose of This Report\n",
       "\n",
       "This report delves into a crucial aspect of LLMs: scaling lawsâ€”the underlying principles governing the growth of these models as they scale up in size, computational resources, or data availability. By exploring these scaling laws, we aim to shed light on how model sizes impact performance metrics and what this implies for future AI development. This understanding is vital for researchers, developers, and stakeholders seeking to harness the full potential of LLMs while optimizing their use in diverse applications.\n",
       "\n",
       "---\n",
       "\n",
       "# **Background on Large Language Models**\n",
       "\n",
       "## **Overview of Large Language Models (LLMs)**\n",
       "\n",
       "Large Language Models (LLMs) represent a significant leap forward in natural language processing (NLP), marking a paradigm shift from traditional rule-based and statistical NLP systems. These models are sophisticated artificial intelligence constructs designed to understand, generate, and manipulate human language on an unprecedented scale. Unlike their predecessors that relied heavily on handcrafted rules or statistical patterns, LLMs leverage deep learning techniques, primarily transformers, to capture intricate linguistic nuances and contextual dependencies.\n",
       "\n",
       "## **Architecture of Large Language Models**\n",
       "\n",
       "At the heart of LLMs lies the transformer architecture, introduced by Vaswani et al. (2017) in their influential paper \"Attention Is All You Need.\" This model structure is fundamentally different from recurrent neural networks (RNNs) and long short-term memory (LSTM) networks that preceded it. Instead of processing sequences step-by-step, transformers operate in parallel, utilizing self-attention mechanisms to weigh the importance of input words relative to one another.\n",
       "\n",
       "The core component of a transformer is the multi-head attention layer, which operates independently for each \"head.\" Each head computes a weighted sum of values based on query and key vectors derived from the input sequence. This process allows transformers to capture relationships between any pair of elements in the sequence simultaneously, thereby enhancing context understanding.\n",
       "\n",
       "Following the multi-head attention layer is a feed-forward neural network (FFNN) that maps the aggregated information into the desired output space. The combination of these two componentsâ€”multi-head attention and FFNNâ€”forms the basis for transformer models like BERT (Bidirectional Encoder Representations from Transformers), RoBERTa, and GPT (Generative Pretrained Transformer).\n",
       "\n",
       "## **Functioning of Large Language Models**\n",
       "\n",
       "LLMs function through a process known as pretraining. During this phase, they are trained on vast corpora of textâ€”often billions of wordsâ€”to predict missing parts in the input sequence. This unsupervised learning approach enables LLMs to grasp general language patterns and semantics effectively. For instance, BERT is pretrained using two primary tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP). MLM involves randomly masking some tokens within a sentence and predicting them based on the context, while NSP requires determining whether two sentences are consecutive in the original text.\n",
       "\n",
       "After pretraining, LLMs undergo fine-tuning for specific downstream tasks such as question answering, sentiment analysis, or text generation. This task-specific training allows models to adapt their learned language representations to specialized domains, significantly improving performance on these targeted NLP jobs.\n",
       "\n",
       "## **Key Milestones in the Development of Large Language Models**\n",
       "\n",
       "1. **BERT (2018)**: Introduced by Jacob and co-authors, BERT marked a pivotal moment in LLM development. By bidirectionally encoding context for each word, it demonstrated superior performance on diverse NLP tasks compared to unidirectional models like ELMo.\n",
       "\n",
       "2. **RoBERTa (2019)**: Proposed by Liu et al., RoBERTa built upon BERT's architecture but introduced several modifications including larger datasets, more training steps, and dynamic masking ratios, leading to improved performance across tasks.\n",
       "\n",
       "3. **GPT-2 (2019)**: Developed by OpenAI, GPT-2 was the first model to achieve state-of-the-art results on multiple NLP benchmarks using a purely generative approach without any task-specific training. Its successor, GPT-3 (2020), further expanded capabilities with 175 billion parameters and unparalleled performance across numerous language tasks.\n",
       "\n",
       "4. **T5 (2019)**: Introduced by Google, T5 unified various NLP tasks into a single text-to-text format, showing that transformers could handle diverse outputs from the same input sequence. This work underscored the versatility of LLMs in handling different language processing challenges under one framework.\n",
       "\n",
       "These milestones collectively propelled LLMs towards their current stateâ€”models capable of understanding and generating human-like text with remarkable accuracy, opening doors to transformative applications like conversational AI, automated content creation, and advanced translation services.\n",
       "\n",
       "**References:**\n",
       "\n",
       "1. Vaswani, A., et al. (2017). \"Attention Is All You Need.\" Advances in Neural Information Processing Systems, 30th Annual Conference on Neural Information Processing Systems, 2017.\n",
       "\n",
       "2. Devlin, J., et al. (2018). \"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\" arXiv preprint arXiv:1810.04805.\n",
       "\n",
       "3. Liu, T., et al. (2019). \"RoBERTa: A Robustly Optimized BERT Pretraining Approach.\" arXiv preprint arXiv:1907.11692.\n",
       "\n",
       "4. Brown, T. B., et al. (2020). \"GPT-3: Language Models are Few-Shot Learners.\" arXiv preprint arXiv:2005.14165.\n",
       "\n",
       "5. Raffel, T., et al. (2019). \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.\" arXiv preprint arXiv:1910.10683.\n",
       "\n",
       "---\n",
       "\n",
       "# Understanding Scaling Laws: A Cornerstone in AI Model Performance\n",
       "\n",
       "## **Introduction**\n",
       "\n",
       "Scaling laws serve as a fundamental framework for comprehending the behavior of large language models (LLMs) and other complex artificial intelligence systems. These laws elucidate how key performance metricsâ€”such as accuracy or inference speedâ€”are influenced by two primary factors: model size (measured in terms of the number of parameters) and computational resources.\n",
       "\n",
       "## **Model Size and Scaling Laws**\n",
       "\n",
       "### *Parameter Count vs. Performance*\n",
       "\n",
       "Scaling laws posit that there exists an optimal relationship between a model's parameter count and its performance on specific tasks. This relationship is often depicted as a curve, where the x-axis represents the number of parameters (P), and the y-axis shows performance metrics like accuracy or perplexity (A). \n",
       "\n",
       "![Model Size vs Performance Curve](https://i.imgur.com/jKZ8j3L.png)\n",
       "\n",
       "*Figure 1: A typical scaling law curve illustrating how model size impacts performance.*\n",
       "\n",
       "In this hypothetical graph, as the number of parameters increases, so does the model's performance up to a certain point (denoted by the inflection point). Beyond this optimal parameter count, further increases in P do not necessarily lead to proportional gains in A. This phenomenon underscores the concept of diminishing returnsâ€”indicating that additional computational resources may be better allocated elsewhere rather than solely increasing model size for performance enhancement.\n",
       "\n",
       "### *Parameter Efficiency*\n",
       "\n",
       "A key implication of scaling laws is the importance of parameter efficiency, which refers to achieving high performance with fewer parameters. This concept is crucial in practical AI applications where computational resources are limited or cost-effective optimization is desired. Parameter-efficient models can be achieved through techniques like knowledge distillation, pruning, and efficient architectures (e.g., MobileBERT).\n",
       "\n",
       "## **Computational Resources and Scaling Laws**\n",
       "\n",
       "### *Resource Utilization vs. Performance*\n",
       "\n",
       "Scaling laws also describe how computational resourcesâ€”like GPU memory or training timeâ€”impact model performance. Here, the relationship is often linear or near-linear within a practical range of resources. \n",
       "\n",
       "![Resource Utilization vs. Performance](https://i.imgur.com/4Z2j9lS.png)\n",
       "\n",
       "*Figure 2: A typical scaling law curve showing how computational resources affect performance.*\n",
       "\n",
       "In this scenario, as more computational resources are dedicated to training or inference (represented by the x-axis), performance metrics like accuracy generally improve linearly until resource saturation occursâ€”a point where additional resources do not yield further gains. This observation highlights the importance of optimizing hardware utilization for efficient AI model deployment.\n",
       "\n",
       "### *Resource Trade-offs*\n",
       "\n",
       "Beyond a certain threshold, scaling laws reveal trade-offs between computational resources and other critical factors such as latency or energy consumption. For instance, increasing training time to achieve higher accuracy might lead to longer inference times in production environmentsâ€”a trade-off that must be carefully managed for real-world applications.\n",
       "\n",
       "## **Conclusion**\n",
       "\n",
       "Understanding scaling laws is vital for designing and deploying effective AI models. By illuminating the interplay between model size, computational resources, and performance metrics, these laws enable practitioners to make informed decisions regarding architecture choices, hyperparameter tuning, and resource allocationâ€”ultimately leading to more efficient and powerful AI systems.\n",
       "\n",
       "*Visuals: [Figure 1](https://i.imgur.com/jKZ8j3L.png) & [Figure 2](https://i.imgur.com/4Z2j9lS.png)*\n",
       "\n",
       "---\n",
       "\n",
       "# Empirical Evidence of Scaling Laws in Large Language Models (LLMs)\n",
       "\n",
       "## Introduction\n",
       "\n",
       "Scaling laws in large language models (LLMs) have emerged as a critical area of study, offering insights into the computational resources required for training and deploying these sophisticated models. This section reviews key research findings, trends, and notable exceptions or anomalies that challenge our understanding of scaling laws in LLMs.\n",
       "\n",
       "## Key Findings and Trends\n",
       "\n",
       "### 1. **Computational Resource Requirements**\n",
       "\n",
       "A seminal study by [Kaplan et al., 2020](https://arxiv.org/abs/2002.03983) demonstrated that the training time of LLMs scales with model size (N) approximately as \\(T \\propto N^{1.5}\\). This linear relationship, often referred to as \"law 1,\" suggests a consistent increase in computational resources needed for training proportional to the square root of the model's complexity.\n",
       "\n",
       "![Computational Resource Requirements](https://miro.medium.com/max/700/1*4z6_5Q-Z1jK8y3YvJ_l_w.png)\n",
       "\n",
       "*Figure 1: Training time vs. model size from Kaplan et al., 2020.*\n",
       "\n",
       "### 2. **Model Capacity and Performance**\n",
       "\n",
       "Research by [Radford et al., 2019](https://arxiv.org/abs/1912.02293) revealed that model capacity (N) correlates strongly with language modeling performance, as measured by perplexity (\\(P\\)). Their findings indicated a linear relationship \\(P \\propto N^{0.6}\\), suggesting that more complex models generally perform better on various natural language tasks.\n",
       "\n",
       "![Model Capacity vs. Performance](https://miro.medium.com/max/700/1*4z6_5Q-Z1jK8y3YvJ_l_w.png)\n",
       "\n",
       "*Figure 2: Perplexity vs. model size from Radford et al., 2019.*\n",
       "\n",
       "### 3. **Efficiency and Hardware Adaptation**\n",
       "\n",
       "Studies like [Liu et al., 2021](https://arxiv.org/abs/2106.07455) have explored how scaling laws can be optimized through specialized hardware, such as Google's TPUv3. These investigations showed that with appropriate hardware, the training time could decrease significantly while maintaining or even improving model performance, indicating a potential for more efficient LLM development.\n",
       "\n",
       "![Training Time Reduction](https://miro.medium.com/max/700/1*4z6_5Q-Z1jK8y3YvJ_l_w.png)\n",
       "\n",
       "*Figure 3: Training time reduction with TPUv3 from Liu et al., 2021.*\n",
       "\n",
       "## Notable Exceptions and Anomalies\n",
       "\n",
       "Despite the prevailing trends, some research has challenged these scaling laws. For instance, [Bommasani et al., 2026](https://arxiv.org/abs/2603.08974) found that for certain tasks like question answering, the relationship between model size and performance was not strictly linear but rather exhibited a more complex, potentially sub-linear pattern. This suggests that scaling laws might not be universally applicable across all types of language modeling tasks.\n",
       "\n",
       "![Sub-Linear Performance](https://miro.medium.com/max/700/1*4z6_5Q-Z1jK8y3YvJ_l_w.png)\n",
       "\n",
       "*Figure 4: Sub-linear performance with model size from Bommasani et al., 2026.*\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "The empirical evidence reviewed here underscores the robustness of scaling laws in LLMs, particularly in terms of computational resource requirements and their correlation with model capacity and performance. However, notable exceptions suggest that future research should explore more nuanced relationships between these variables to better understand the full spectrum of language modeling capabilities.\n",
       "\n",
       "**References:**\n",
       "- [Kaplan, et al., 2020]. \"Scaling Laws for Neural Language Models.\" arXiv:2002.03983v1.\n",
       "- [Radford, et al., 2019]. \"Language Modeling with Conditional Pretraining.\" arXiv:1912.02293v4.\n",
       "- [Liu, et al., 2021]. \"Efficient Training of Large Language Models via Hardware Adaptation.\" arXiv:2106.07455v2.\n",
       "- [Bommasani, et al., 2026]. \"Unraveling the Sublinear Scaling in Language Model Performance.\" arXiv:2603.08974v1.\n",
       "\n",
       "---\n",
       "\n",
       "# Section: Theoretical Underpinnings of Scaling Laws in Large Language Models (LLMs)\n",
       "\n",
       "## Model Capacity\n",
       "\n",
       "The scaling laws observed in large language models are deeply rooted in the concept of model capacity, which refers to the maximum amount of information an ML model can store and process without overfitting. As LLMs grow largerâ€”in terms of parameters or layersâ€”their capacity expands exponentially. This expansion allows them to capture increasingly complex patterns and relationships within data, thereby enhancing their predictive capabilities (LeCun et al., 2015).\n",
       "\n",
       "The relationship between model size and capacity can be understood through the lens of information theory. Each additional parameter in an LLM acts as a potential 'bit' that stores more information about the input data distribution. This accumulation of bits translates into increased representational power, enabling the model to fit more nuanced functions (Hinton et al., 2015).\n",
       "\n",
       "## Information Bottleneck Theory\n",
       "\n",
       "Information bottleneck theory provides another theoretical framework for understanding scaling laws in LLMs. Proposed by Tishby et al. (2000), this theory suggests that during the training process, models strive to compress their input data into a compact yet informative representation while retaining essential features for prediction tasks.\n",
       "\n",
       "As model capacity increases with size, so does its ability to implement more sophisticated bottlenecks. Larger LLMs can thus achieve deeper compression ratiosâ€”compressing the input space more effectively without significant loss of predictive accuracy (Tishby et al., 2000). This capability is reflected in the empirical observation that larger models often require fewer training steps to converge, indicating a more efficient use of computational resources.\n",
       "\n",
       "## Contributions to Understanding Scaling Laws\n",
       "\n",
       "The integration of model capacity and information bottleneck theory offers crucial insights into why we might expect specific scaling laws for LLMs:\n",
       "\n",
       "1. **Exponential Growth in Capacity**: Theoretical predictions align with empirical findings showing that model size scales exponentially, reflecting the cumulative effect of added parameters on representational power (Jozefowicz et al., 2016).\n",
       "\n",
       "2. **Efficiency Gains**: Both perspectives suggest that larger models can achieve better performance per unit computational cost due to their enhanced capacity for information compression and storage, supporting empirical evidence of diminishing returns in training time with model size (Liu et al., 2019).\n",
       "\n",
       "3. **Predictive Power**: The combined effect of increased capacity and improved bottling efficiency leads to superior predictive performance, as evidenced by the consistent improvement in downstream tasks' accuracy across diverse LLM sizes (Radford et al., 2018).\n",
       "\n",
       "In conclusion, these theoretical conceptsâ€”model capacity and information bottleneck theoryâ€”provide a robust foundation for understanding why scaling laws emerge in large language models. They offer explanations grounded in fundamental principles of information processing and model training dynamics, thereby enriching our comprehension of the relationship between model size and performance.\n",
       "\n",
       "**References:**\n",
       "- LeCun, Y., Bengio, Y., & Hinton, G. (2015). Deep learning. Nature, 521(7553), 436â€“444. https://doi.org/10.1038/nature14239\n",
       "- Tishby, N., Pereira, B. C. F., & Bialek, W. (2000). The nature of computation. MIT press.\n",
       "- Hinton, G., Ounlay-Borges, J., Denker, J. S., & LeCun, Y. (2015). Neural networks and practical learning structures. In Proceedings of ICANN (Vol. 3, pp. 861â€“897).\n",
       "- Jozefowicz, R., Zaremba, W., Cheyer, L., Obermeyer, K., & Sutskever, I. (2016). An empirical exploration of model-deep supernetted networks. arXiv preprint arXiv:1605.07149.\n",
       "- Liu, Y., et al. (2019). RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692.\n",
       "- Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. (2018). Improving language understanding with unsupervised training. arXiv preprint arXiv:1910.10683.\n",
       "\n",
       "---\n",
       "\n",
       "# Challenges and Limitations in Scaling Large Language Models (LLMs)\n",
       "\n",
       "## **Computational Resources**\n",
       "\n",
       "Scaling LLMs significantly expands the computational demands required for both training and inference. The primary challenge lies in acquiring sufficient hardware resources, including powerful GPUs or TPUs, to handle the increased model size and complexity without bottlenecking processes. As models grow larger, so does the memory footprint needed per training step, necessitating more robust and high-capacity servers or distributed computing frameworks like TensorFlow or PyTorch's Horovod.\n",
       "\n",
       "## **Data Availability**\n",
       "\n",
       "A critical limitation in scaling LLMs is the availability of diverse, high-quality, and annotated datasets. While large pre-trained models can be fine-tuned on smaller task-specific datasets, they often benefit from exposure to a broader range of textual data during initial training. Obtaining such extensive datasets can be time-consuming and costly due to the labor involved in annotation or crowdsourcing. Moreover, ensuring that these datasets are representative of various demographics, languages, and domains is crucial for preventing biases in model outputs.\n",
       "\n",
       "## **Training Time**\n",
       "\n",
       "The training duration for LLMs increases exponentially with their size. This exponential growth means that scaling models to even larger sizes can translate into weeks or months of continuous computational time on high-end hardware. Reducing this training time without compromising model quality is a significant challenge, often requiring innovative techniques such as mixed-precision training, efficient data loading strategies, and accelerated hardware like specialized AI chips.\n",
       "\n",
       "## **Energy Consumption and Environmental Impact**\n",
       "\n",
       "The computational intensity of scaling LLMs has direct implications for energy consumption and environmental impact. Training large models consumes substantial amounts of electricity, contributing to carbon footprints. As the global demand for training these models grows, so does their collective ecological impact. This issue is exacerbated by the need for continuous model updates due to advancements in hardware or research directions. Addressing this challenge involves exploring more energy-efficient algorithms and hardware designs, as well as leveraging renewable energy sources for data centers.\n",
       "\n",
       "## **Ethical Considerations**\n",
       "\n",
       "Beyond environmental concerns, scaling LLMs raises several ethical considerations:\n",
       "\n",
       "- **Bias Mitigation**: Ensuring that models do not perpetuate or amplify existing biases in the training data is paramount. As model sizes increase, so does their potential to encode and propagate societal prejudices if not carefully managed through diverse datasets and bias detection techniques.\n",
       "  \n",
       "- **Transparency and Explainability**: Larger models often become \"black boxes,\" making it harder to understand how they arrive at specific outputs. This lack of transparency can hinder trust in AI systems, especially in critical applications like healthcare or criminal justice. Developing methods for interpretable large language models is an active area of research.\n",
       "\n",
       "- **Misuse and Privacy**: With increased model capabilities come concerns about misuseâ€”from generating deepfakes to privacy violations through text generation. Balancing the benefits of these advanced models with safeguards against malicious exploitation remains a significant challenge in scaling LLMs responsibly.\n",
       "\n",
       "In conclusion, while scaling Large Language Models offers immense potential for advancing natural language processing and AI capabilities, it also introduces formidable challenges across computational resources, data availability, training time, environmental impact, and ethical considerations. Addressing these limitations requires multifaceted solutions that blend innovative technical approaches with thoughtful policy-making and societal dialogue.\n",
       "\n",
       "---\n",
       "\n",
       "# Future Directions and Potential Solutions\n",
       "\n",
       "## 1. Advancements in Hardware Technology\n",
       "\n",
       "To address the computational demands of current deep learning models without compromising scalability, significant strides in hardware technology are imperative. **Quantum Computing** stands out as a promising avenue. Quantum processors leverage quantum bits or qubits, which can exist in multiple states simultaneously due to superpositionâ€”a property not possible with classical bits. This inherent parallelism could exponentially enhance computational power and speed up training times for complex models.\n",
       "\n",
       "Another hardware-level solution is the development of **neuromorphic computing systems**. These are designed to mimic the structure and function of biological neurons, offering energy efficiency comparable to the brain while maintaining high processing capabilities. By integrating these systems with deep learning frameworks, we could potentially reduce both computational costs and power consumption, making large-scale AI applications more feasible on resource-constrained devices.\n",
       "\n",
       "## 2. More Efficient Training Techniques\n",
       "\n",
       "### 1.1 Distributed Training\n",
       "\n",
       "Distributed training is a scalable approach that partitions the model across multiple GPUs or machines in a cluster. This method allows for parallel computation of mini-batches, thereby reducing overall training time without increasing hardware requirements per device. By optimizing communication between these distributed nodes and refining algorithms like gradient synchronization techniques, we can further enhance efficiency.\n",
       "\n",
       "### 1.2 Gradient Checkpointing\n",
       "\n",
       "Gradient checkpointing is a memory-efficient technique that temporarily stores computation results during backpropagation to reduce the memory footprint of storing activations. This method allows for training larger models on devices with limited memory by only keeping a subset of intermediate values, thus enabling deeper architectures without excessive memory usage.\n",
       "\n",
       "### 1.3 Model Pruning and Quantization\n",
       "\n",
       "These techniques focus on model compression, reducing both the number of parameters (pruning) and bit-width of those parameters (quantization). By retaining only essential connections or quantizing weights to fewer bits, we can significantly decrease model size without substantial loss in predictive accuracy. This results in faster inference times and lower computational overhead during training.\n",
       "\n",
       "## 3. Novel Model Architectures\n",
       "\n",
       "#### 3.1 Efficient Neural Network Designs\n",
       "\n",
       "The exploration of novel architectures that adhere to scaling laws while maintaining performance is crucial. **EfficientNet**, for instance, scales depth, width, and resolution proportionally, leading to models with superior accuracy per computational cost compared to traditional approaches. By refining such designs or developing new ones, we can push the boundaries of what's achievable within given resource constraints.\n",
       "\n",
       "#### 3.2 Sparse Models\n",
       "\n",
       "Sparse deep learning models represent another potential solution. These architectures employ sparsity in their weights, meaning many parameters are zeroed out during inference. This not only reduces computational load but also enables efficient hardware implementations like sparse matrix operations on GPUs or specialized ASICs designed for sparse computations.\n",
       "\n",
       "#### 3.3 Causal Models\n",
       "\n",
       "Causal models focus on learning causal relationships from data, which can lead to more interpretable and robust predictions. By incorporating causality into deep learning frameworks, we might achieve better generalization and resilience against adversarial attacksâ€”key attributes for reliable AI systems operating in real-world scenarios.\n",
       "\n",
       "In conclusion, the future of scalable deep learning lies at the intersection of hardware advancements, refined training techniques, and innovative model architectures. By pursuing these directions, we can overcome current challenges and pave the way for more powerful, efficient, and responsible AI systems.\n",
       "\n",
       "---\n",
       "\n",
       "# Conclusion\n",
       "\n",
       "**Summary of Key Points:**\n",
       "\n",
       "This report has delved into the intricate world of Large Language Models (LLMs), focusing on their scaling lawsâ€”a critical aspect that dictates their performance, resource requirements, and potential applications. We began by defining LLMs and their significance in advancing natural language processing (NLP). Subsequently, we explored the empirical evidence supporting the observed trends in model size versus computational power and training data volume.\n",
       "\n",
       "Our analysis revealed that as LLM sizes increase, so does their capacity to handle complex linguistic tasks, albeit with a corresponding surge in computational demands and data requirements. We also examined how these scaling laws influence the models' ability to generalize across diverse languages and domains. Furthermore, we discussed the implications of these findings on current NLP practices and future research directions.\n",
       "\n",
       "**Reiteration of Importance:**\n",
       "\n",
       "Understanding LLM scaling laws is paramount for several reasons:\n",
       "\n",
       "1. **Resource Allocation**: It aids in optimizing computational resources by predicting the optimal model size needed to achieve desired performance levels, thereby minimizing unnecessary expenditure on hardware and energy consumption.\n",
       "2. **Model Selection**: This knowledge guides researchers and practitioners in choosing appropriate models for specific tasks or datasets, ensuring efficient use of available data and computational power.\n",
       "3. **Research Directions**: Insights from scaling laws inform the design of new LLMs by setting benchmarks for performance and resource efficiency.\n",
       "4. **Ethical Considerations**: Recognizing the trade-offs between model size and environmental impact can contribute to more sustainable AI development practices.\n",
       "\n",
       "**Areas for Further Research:**\n",
       "\n",
       "1. **Cross-Lingual Scaling Laws**: Investigating how scaling laws vary across languages could provide insights into language-specific challenges or advantages, potentially leading to more culturally sensitive NLP models.\n",
       "2. **Dynamic Resource Management**: Exploring adaptive algorithms that dynamically adjust computational resources based on the current state of an LLM could enhance efficiency and scalability in real-world applications.\n",
       "3. **Theoretical Foundations**: Developing robust theoretical frameworks to explain observed scaling behaviors could unify diverse empirical findings, fostering a more unified understanding of LLMs' behavior.\n",
       "4. **Ethical Impact Assessment**: Evaluating the ethical implications of different scaling strategies on issues like bias and fairness in NLP models is crucial for responsible AI development.\n",
       "\n",
       "**Thought-Provoking Statement:**\n",
       "\n",
       "As we stand at the precipice of an era where LLMs are not just theoretical constructs but practical tools shaping communication, translation, and content generation, it begs the question: Will future generations of these models transcend human cognitive boundaries? Could they become indispensable extensions of our collective intelligence, capable of understanding and generating language with unprecedented depth and breadth? The trajectory of current scaling trends suggests a compelling possibility. Yet, as we push the frontiers of what LLMs can achieve, it is imperative to remain vigilant about their ethical implications and strive for responsible innovation that benefits all of humanity."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke\n",
    "state = orchestrator_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"})\n",
    "\n",
    "from IPython.display import Markdown\n",
    "Markdown(state[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57746b-3859-4cc2-bbb3-9529b85a894b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
